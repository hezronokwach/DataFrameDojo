{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_np = np.array([\n",
    "    ['Blue', [1, 2], 1.1],\n",
    "    ['Red', [3, 4], 2.2],\n",
    "    ['Pink', [5, 6], 3.3],\n",
    "    ['Grey', [7, 8], 4.4],\n",
    "    ['Black', [9, 10], 5.5]\n",
    "], dtype=object)\n",
    "\n",
    "ecommerce_from_numpy = pd.DataFrame(data_np, index=[1, 3, 5, 7, 9],\n",
    "                             columns=['color', 'list', 'number'])\n",
    "\n",
    "print(\"DataFrame from NumPy array:\")\n",
    "print(ecommerce_from_numpy)\n",
    "\n",
    "# Create DataFrame from Pandas Series\n",
    "data_series = {\n",
    "    'color': pd.Series(['Blue', 'Red', 'Pink', 'Grey', 'Black'],\n",
    "                       index=[1, 3, 5, 7, 9]),\n",
    "    'list': pd.Series([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]],\n",
    "                      index=[1, 3, 5, 7, 9]),\n",
    "    'number': pd.Series([1.1, 2.2, 3.3, 4.4, 5.5], index=[1, 3, 5,\n",
    "                                                              7, 9])\n",
    "}\n",
    "\n",
    "ecommerce_from_series = pd.DataFrame(data_series)\n",
    "\n",
    "print(\"DataFrame from Pandas Series:\")\n",
    "print(ecommerce_from_series)\n",
    "\n",
    "# Print column types\n",
    "print(\"Column types:\")\n",
    "print(ecommerce_from_numpy.dtypes)\n",
    "\n",
    "# Print types of the first value of every column\n",
    "print(\"Types of the first value of every column:\")\n",
    "for col in ecommerce_from_numpy.columns:\n",
    "    print(f\"Column '{col}': {type(ecommerce_from_numpy[col].iloc[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46db3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading, Cleaning, and Transformation ---\n",
    "\n",
    "# 1. Load the dataset\n",
    "print(\"Step 1: Loading data...\")\n",
    "file_path = 'household_power_consumption_2.txt'\n",
    "ecommerce = pd.read_csv(file_path, sep=';', na_values=['?'])\n",
    "ecommerce = ecommerce.drop(columns=['Time', 'Sub_metering_2', 'Sub_metering_3'])\n",
    "ecommerce = ecommerce.set_index('Date')\n",
    "\n",
    "# 2. Update data types\n",
    "print(\"Step 2: Updating data types...\")\n",
    "def update_types(ecommerce_to_update):\n",
    "    for col in ecommerce_to_update.columns:\n",
    "        ecommerce_to_update[col] = pd.to_numeric(ecommerce_to_update[col], errors='coerce')\n",
    "    ecommerce_to_update.index = pd.to_datetime(ecommerce_to_update.index, format='%d/%m/%Y')\n",
    "    return ecommerce_to_update\n",
    "ecommerce_updated = update_types(ecommerce)\n",
    "\n",
    "ecommerce_updated.describe()\n",
    "\n",
    "# 3. Drop rows with missing values and create a copy\n",
    "print(\"Step 3: Dropping missing values...\")\n",
    "ecommerce_cleaned = ecommerce_updated.dropna().copy()\n",
    "\n",
    "print(\"--- Sub_metering_1 before modification ---\")\n",
    "print(ecommerce_cleaned['Sub_metering_1'].head())\n",
    "\n",
    "# 4. Apply the transformation\n",
    "print(\"Step 4: Applying transformation...\")\n",
    "ecommerce_cleaned['Sub_metering_1'] = (ecommerce_cleaned['Sub_metering_1'] + 1) * 0.06\n",
    "\n",
    "print(\"--- Sub_metering_1 after modification ---\")\n",
    "print(ecommerce_cleaned['Sub_metering_1'].head())\n",
    "\n",
    "  # 1. Select rows where Date >= 2008-12-27 and Voltage >= 242\n",
    "print(\"--- 1. Filtering Data ---\")\n",
    "filtered_ecommerce = ecommerce_cleaned[(ecommerce_cleaned.index >= '2008-12-27') & (ecommerce_cleaned['Voltage'] >= 242)]\n",
    "print(f\"Found {len(filtered_ecommerce)} rows matching the criteria.\")\n",
    "\n",
    "print(\"\\n--- 2. 88888th Row of Filtered Data ---\")\n",
    "if len(filtered_ecommerce) > 88888:\n",
    "    print(filtered_ecommerce.iloc[88888])\n",
    "else:\n",
    "    print(\"There are not enough rows in the filtered data to select the 88888th row.\")\n",
    "\n",
    "\n",
    "  # 3. Find the date of the maximum Global_active_power\n",
    "print(\"\\n--- 3. Date of Maximum Global Active Power ---\")\n",
    "max_power_date = ecommerce_cleaned['Global_active_power'].idxmax()\n",
    "print(f\"The Global_active_power was maximal on: {max_power_date.date()}\")\n",
    "\n",
    "\n",
    "  # 4. Sort the first three columns\n",
    "print(\"\\n--- 4. Sorted DataFrame (First 3 Columns) ---\")\n",
    "sorted_ecommerce = ecommerce_cleaned.sort_values(by=['Global_active_power','Voltage'], ascending=[False, True])\n",
    "print(sorted_ecommerce.iloc[:, :3].head()) # Displaying the first 3 columns of the sorted result\n",
    "\n",
    "\n",
    "  # 5. Compute the daily average of Global_active_power\n",
    "print(\"\\n--- 5. Daily Average of Global Active Power ---\")\n",
    "daily_avg_power = ecommerce_cleaned['Global_active_power'].resample('D').mean()\n",
    "print(daily_avg_power.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059aae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce = pd.read_csv(\"Ecommerce_purchases.txt\", sep=',')\n",
    "#print(ecommerce.head())\n",
    "\n",
    "print(f\"Total Rows = {len(ecommerce.index)}\")\n",
    "print(f\"Total Rows = {len(ecommerce.columns)}\")\n",
    "\n",
    "purchase_price = np.mean(ecommerce['Purchase Price'])\n",
    "print(f\"Average Purchase Price = {purchase_price}\")\n",
    "\n",
    "english_speakers = ecommerce['Language']\n",
    "num_english_speakers = len(english_speakers[english_speakers == 'en'])\n",
    "print(f\"Number of English Speakers = {num_english_speakers}\")\n",
    "\n",
    "jobs = ecommerce['Job']\n",
    "num_lawyers = len(jobs[jobs == 'Lawyer'])\n",
    "print(f\"Number of Lawyers = {num_lawyers}\")\n",
    "\n",
    "am_buyers = ecommerce['AM or PM']\n",
    "num_am_buyers = len(am_buyers[am_buyers == 'AM'])\n",
    "print(f\"Number of AM Buyers = {num_am_buyers}\")\n",
    "\n",
    "num_pm_buyers = len(am_buyers[am_buyers == 'PM'])\n",
    "print(f\"Number of PM Buyers = {num_pm_buyers}\")\n",
    "\n",
    "top_jobs = jobs.value_counts()\n",
    "# print(f\"Top jobs {top_jobs.head(5)}\")\n",
    "\n",
    "# Step 1: Create a boolean \"mask\" to find which row has the Lot '90 WT'\n",
    "# This will be a Series of True/False values.\n",
    "is_the_correct_lot = ecommerce['Lot'] == '90 WT'\n",
    "\n",
    "# Step 2: Use the mask to select the entire row from the DataFrame.\n",
    "# .loc is great for selecting data by labels or boolean conditions.\n",
    "transaction_row = ecommerce.loc[is_the_correct_lot]\n",
    "\n",
    "# Step 3: From that specific row, select the value in the 'Purchase Price' column.\n",
    "# We use .item() to pull the single value out of the Series.\n",
    "purchase_price = transaction_row['Purchase Price'].item()\n",
    "print(f\"Purchase Price for Lot '90 WT' = {purchase_price}\")\n",
    "\n",
    "credit_card_to_find = 4926535242672853\n",
    "email_address = ecommerce.loc[ecommerce['Credit Card'] == credit_card_to_find, 'Email'].item()\n",
    "\n",
    "print(f\"Email of the person with Credit Card number '4926535242672853' = {email_address}\")\n",
    "\n",
    "american_express = ecommerce[(ecommerce['CC Provider'] == 'American Express') & (ecommerce['Purchase Price'] >= 95.0)]\n",
    "print(f'Total american express above $95 {len(american_express)}')\n",
    "\n",
    "\n",
    "expiry = ecommerce['CC Exp Date'].str.endswith('25')\n",
    "num_expiring_2025 = expiry.sum()\n",
    "print(f'Cards expiring in 2025 = {num_expiring_2025}')\n",
    "\n",
    "\n",
    "ecommerce.columns = ecommerce.columns.str.strip()\n",
    "\n",
    "# --- Step 1: Extract the domain from each email address ---\n",
    "# We use the .str accessor to apply string operations to the 'Email' column.\n",
    "# .split('@') splits each email into a list of two parts: the name and the domain.\n",
    "# .str[1] selects the second part of that list, which is the domain.\n",
    "email_providers = ecommerce['Email'].str.split('@').str[1]\n",
    "\n",
    "# --- Step 2: Count the occurrences of each provider ---\n",
    "# value_counts() is the perfect tool for this. It counts unique values and\n",
    "# sorts them in descending order automatically.\n",
    "provider_counts = email_providers.value_counts()\n",
    "\n",
    "# --- Step 3: Get the top 5 most popular providers ---\n",
    "# .head(5) selects the first 5 rows from the sorted counts.\n",
    "top_5_providers = provider_counts.head(5)\n",
    "\n",
    "print(\"Top 5 most popular email providers:\")\n",
    "print(top_5_providers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Medians of the columns:\n",
      "sepal_length    5.8\n",
      "sepal_width     3.0\n",
      "petal_length    4.0\n",
      "petal_width     1.3\n",
      "dtype: float64\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris_data = pd.read_csv('iris.csv', sep= ',')\n",
    "iris_data.drop(columns=['flower'])\n",
    "print(iris_data.head(5))\n",
    "\n",
    "\n",
    "columns_to_convert = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "iris_data[columns_to_convert] = iris_data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "iris_data['sepal_length'] = pd.to_numeric(iris_data['sepal_length'], errors='coerce')\n",
    "\n",
    "\n",
    "sepal_length_mean = iris_data['sepal_length'].mean()\n",
    "iris_data.loc[iris_data['sepal_length'].isnull(), 'sepal_length'] = sepal_length_mean\n",
    "\n",
    "sepal_width_median = iris_data['sepal_width'].median()\n",
    "iris_data.loc[iris_data['sepal_width'].isnull(), 'sepal_width'] = sepal_width_median\n",
    "\n",
    "columns_to_zero = ['petal_length', 'petal_width']\n",
    "for col in columns_to_zero:\n",
    "    iris_data.loc[iris_data[col].isnull(), col] = 0\n",
    "    \n",
    "\n",
    "# Calculate the median for each specified column\n",
    "column_medians = iris_data[columns_to_convert].median()\n",
    "print(\"\\nMedians of the columns:\")\n",
    "print(column_medians)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Use fillna() to replace NaN values with the calculated medians\n",
    "iris_data[columns_to_convert] = iris_data[columns_to_convert].fillna(column_medians)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47b523",
   "metadata": {},
   "source": [
    "# Handling Missing Values in Pandas: Why Mean or Zero Imputation Can Be Problematic\n",
    "\n",
    "When dealing with missing values in a dataset, simple imputation methods like filling with `0` or the column mean can seem appealing due to their simplicity. However, these approaches often introduce significant issues, especially when encountering problematic data like the \"special row\" (`122,always,check,the,data,!!!!!!!!`). This document explains why mean or zero imputation is inappropriate, particularly in the context of such a row, and outlines a better approach to handle missing or corrupted data.\n",
    "\n",
    "## Why Filling Missing Values with 0 or the Mean is a Bad Idea\n",
    "\n",
    "### 1. Mean or Zero Imputation is Inappropriate for Categorical/Mixed Data\n",
    "- **Data Type Mismatch**: The \"special row\" (`122,always,check,the,data,!!!!!!!!`) is a string, not a numeric value. Pandas cannot compute a mean for a column containing such a value and would raise a `TypeError`. Even if you attempt to convert it using `pd.to_numeric(..., errors='coerce')`, the entry becomes `NaN`, indicating that the original information is not numeric. Imputing with `0` or the mean in this case would be meaningless and fail to address the underlying issue.\n",
    "- **Loss of Information**: Replacing the \"special row\" with `0` or the mean erases critical evidence of a data quality issue. This row acts as an internal alarm, signaling a failure in data collection, cleaning, or parsing. Imputing with a numeric value hides this problem, potentially allowing broader data quality issues to go unnoticed and propagate through the analysis.\n",
    "\n",
    "### 2. Mean or Zero Imputation Biases the Data\n",
    "- **Distorts Distribution**: If you extract a numeric part from the \"special row\" (e.g., `122`) and impute missing values with it, you risk introducing an outlier that skews the data distribution. Conversely, imputing with `0` in a dataset with large positive values (e.g., sepal lengths) artificially shifts the distribution downward, misrepresenting the data's true range and central tendency.\n",
    "- **Underestimates Variance**: Mean imputation reduces the variance of a column by replacing missing values with a single fixed value (the mean). This creates an overly optimistic view of the data's consistency, which can lead to incorrect statistical inferences or hypothesis tests. For example, variance-based analyses (e.g., standard deviation, confidence intervals) will be biased, affecting model performance.\n",
    "\n",
    "### 3. It's an Unsuitable Solution for Context-Dependent Problems\n",
    "- **Ignores Missingness Mechanism**: The \"special row\" represents a **Missing Not at Random (MNAR)** scenario, where the missing or corrupted data is related to the value itself (e.g., due to human error or process failure). Mean or zero imputation assumes data is **Missing Completely at Random (MCAR)**, which is inappropriate here. A model trained on such imputed data may fail to learn the true patterns, leading to poor performance in real-world applications.\n",
    "- **Loss of Relationships**: For models relying on feature relationships (e.g., regression), mean imputation disrupts correlations by assigning all missing values the same number. This weakens or destroys the relationships between features, reducing the model's ability to capture meaningful patterns.\n",
    "\n",
    "## How to Handle the Special Row Properly\n",
    "\n",
    "For a data quality issue as blatant as the \"special row,\" simple imputation is the wrong approach. A robust data cleaning process is necessary to address such issues effectively. Below is a step-by-step strategy to handle the \"special row\" and other missing values:\n",
    "\n",
    "1. **Isolate the Problem Row**  \n",
    "   Identify rows containing malformed data, such as the \"special row.\" For example, you can search for rows containing specific string patterns.\n",
    "\n",
    "   ```python\n",
    "   bad_data_row = df[df['sepal_length'].astype(str).str.contains(\"always\")]\n",
    "   ```\n",
    "\n",
    "2. **Inspect and Decide on an Action**  \n",
    "   Upon inspection, confirm that the row is invalid and cannot be salvaged. The \"special row\" (`122,always,check,the,data,!!!!!!!!`) is clearly erroneous and does not represent a valid data point.\n",
    "\n",
    "3. **Remove the Problematic Row**  \n",
    "   Drop the row from the DataFrame to eliminate the corrupted data. This is the safest approach when the data is unrecoverable.\n",
    "\n",
    "   ```python\n",
    "   df_cleaned = df.drop(bad_data_row.index)\n",
    "   ```\n",
    "\n",
    "4. **Fill Remaining Missing Values**  \n",
    "   After removing the problematic row, apply a robust imputation strategy for any remaining legitimately missing values. For numeric columns, using the median is often more appropriate than the mean, as it is less sensitive to outliers.\n",
    "\n",
    "   ```python\n",
    "   df_cleaned = df_cleaned.fillna(df_cleaned.median(numeric_only=True))\n",
    "   ```\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Filling missing values with `0` or the mean is a poor choice for datasets with corrupted or context-dependent missing data, such as the \"special row.\" These methods can distort distributions, underestimate variance, and hide critical data quality issues. Instead, a careful cleaning process—identifying and removing erroneous data followed by appropriate imputation for legitimate missing values—ensures a more accurate and reliable dataset for analysis. By addressing data quality issues explicitly, you preserve the integrity of your analysis and improve the performance of downstream models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex00",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
